# after exporting the follower lists with xExport extension and saving them to a xlsx file
!pip install pandas openpyxl
!pip install xlsxwriter
import pandas as pd
from collections import Counter
from google.colab import files  # Only needed if in Colab
uploaded = files.upload()

# Read all sheets
sheets = pd.read_excel("C1-right-ids.xlsx", sheet_name=None, engine='openpyxl')
print(sheets.keys())  # Print all sheet names

# after generating list of French locations from the follower list and saving it to txt file
from google.colab import files
uploaded = files.upload()

# load the Excel file and French locations list
xls = pd.ExcelFile("C1-right-ids.xlsx")
sheet_names = xls.sheet_names

with open("french_locations.txt", "r", encoding="utf-8") as f:
    french_cities_set = set(line.strip() for line in f.readlines())

# Filter each sheet
filtered_dataframes = {}

for sheet in sheet_names:
    # Read all columns as strings
    df = xls.parse(sheet, dtype=str)

    if 'Location' in df.columns:
        df['Location_cleaned'] = df['Location'].astype(str).str.strip()
        filtered_df = df[df['Location_cleaned'].isin(french_cities_set)]
        filtered_dataframes[sheet] = filtered_df.drop(columns=['Location_cleaned'])

# Save filtered data to a new Excel file
output_path = "filtered_french_locations.xlsx"
with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
    for sheet_name, df in filtered_dataframes.items():
        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)

# Download the filtered Excel file
files.download(output_path)

# Remove those with 0 tweets In all the sheets of both Excel files, only keep the rows which include a number higher than 0 in column H. Remove all blank rows to make it a consecutive list.
# Load the filtered Excel file
input_file = "filtered_french_locations.xlsx"
xls = pd.ExcelFile(input_file)

# Prepare a dictionary to store cleaned dataframes
cleaned_sheets = {}

# Process each sheet
for sheet_name in xls.sheet_names:
    df = xls.parse(sheet_name, dtype=str)

    # Ensure there are at least 8 columns (column H is the 8th)
    if df.shape[1] >= 8:
        # Keep rows where column H is a number and greater than 0
        df = df[pd.to_numeric(df.iloc[:, 7], errors='coerce').fillna(0) > 0]

    # Drop rows that are completely blank
    df = df.dropna(how='all')
    cleaned_sheets[sheet_name] = df

# Save the cleaned data to a new Excel file
output_file = "C1_filtered_followers_with-tweets.xlsx"
with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
    for sheet, data in cleaned_sheets.items():
        data.to_excel(writer, sheet_name=sheet[:31], index=False)

print(f"Cleaned file saved as: {output_file}")

# Download the filtered Excel file for inspection
files.download(output_file)

# In far-right-corpus, only keep users who follow two far-right X accounts
# In all the sheets of far-right Excel file, count all the duplicates in column C.
# Remove all the rows which occur only once in the whole spreadsheet

# Load the Excel file
input_file = "far-right_filtered_followers.xlsx"
xls = pd.ExcelFile(input_file)

# Collect all values from column C across all sheets
col_c_values = []

for sheet_name in xls.sheet_names:
    df = xls.parse(sheet_name, dtype=str)
    if df.shape[1] >= 3:  # Ensure column C exists
        col_c_values.extend(df.iloc[:, 2].dropna().astype(str).tolist())

# Count occurrences of each value in column C
c_value_counts = Counter(col_c_values)

# Step 3: Filter each sheet to keep only rows where column C value occurs > 1
filtered_sheets = {}

for sheet_name in xls.sheet_names:
    df = xls.parse(sheet_name, dtype=str)
    if df.shape[1] >= 3:
        col_c = df.iloc[:, 2].astype(str)
        mask = col_c.map(lambda val: c_value_counts[val] > 1)
        df_filtered = df[mask]
        filtered_sheets[sheet_name] = df_filtered
    else:
        filtered_sheets[sheet_name] = df  # If no column C, keep original

# Step 4: Save to a new Excel file
output_file = "far-right_duplicates_only_column.xlsx"
with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
    for sheet_name, df in filtered_sheets.items():
        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)

print(f"Filtered file saved as: {output_file}")
files.download(output_file)

# Merge all sheets into one list of unique rows and remove duplicates.
# Load the Excel file
input_file = "far-right_duplicates_only_all_columns.xlsx"
xls = pd.ExcelFile(input_file)

# Combine all sheets
all_data = []

for sheet_name in xls.sheet_names:
    df = xls.parse(sheet_name, dtype=str)
    all_data.append(df)

combined_df = pd.concat(all_data, ignore_index=True)

# Remove duplicate rows (based on all columns)
unique_df = combined_df.drop_duplicates()

# Save to a new Excel file
output_file = "far-right_followers.xlsx"
unique_df.to_excel(output_file, index=False)

print(f"Merged and de-duplicated file saved as: {output_file}")
files.download (output_file)
