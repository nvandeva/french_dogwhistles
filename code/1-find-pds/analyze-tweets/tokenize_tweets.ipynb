{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Remove punctuation and stop words\n",
        "- Tokenize the cleaned tweets\n",
        "- Compute token count, type count, and token-to-type ratio (TTR)\n",
        "- Lemmatize each token\n",
        "- Count lemma frequency\n",
        "- Show the 20 most common lemmas"
      ],
      "metadata": {
        "id": "v0D-a4FLjUrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcn9Zq3Di5r3",
        "outputId": "eb706338-5c69-43cb-9b70-fa4e17f92520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EfaC_K9irrv",
        "outputId": "8ab6b02a-cb90-472a-84f7-fbc907490372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0M5r_DkULfA",
        "outputId": "0957ed46-d233-496a-af55-1e3696e4797c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization and Lemmatization Report\n",
            "Total tokens (excluding stopwords, punctuation, numbers): 1434864\n",
            "Unique tokens (types): 92355\n",
            "Token-to-Type Ratio (TTR): 15.54\n",
            "\n",
            "Top 20 Most Frequent Lemmas:\n",
            "faire: 10012\n",
            "√™tre: 9664\n",
            "bien: 8731\n",
            "france: 6359\n",
            "falloir: 5919\n",
            "non: 5753\n",
            "bon: 5713\n",
            "voir: 5687\n",
            "rien: 5236\n",
            "vouloir: 5043\n",
            "contre: 4536\n",
            "an: 4262\n",
            "fran√ßais: 4228\n",
            "oui: 4186\n",
            "monde: 3687\n",
            "savoir: 3675\n",
            "politique: 3614\n",
            "pouvoir: 3609\n",
            "avoir: 3594\n",
            "prendre: 3582\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load French spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Read cleaned French tweets\n",
        "df = pd.read_csv('/content/drive/My Drive/_thesis_dogwhistles/Analyze-tweets/left-cleaned-french-tweets.csv')\n",
        "\n",
        "# Prepare text for spaCy processing\n",
        "texts = df['content'].dropna().astype(str).tolist()\n",
        "\n",
        "# Tokenize, remove stopwords, punctuation, numbers, and lemmatize\n",
        "all_tokens = []\n",
        "all_lemmas = []\n",
        "\n",
        "for doc in nlp.pipe(texts, disable=[\"ner\", \"parser\"]):\n",
        "    for token in doc:\n",
        "        if token.is_alpha and not token.is_stop:\n",
        "            all_tokens.append(token.text.lower())\n",
        "            all_lemmas.append(token.lemma_.lower())\n",
        "\n",
        "# Count tokens, types, TTR\n",
        "total_tokens = len(all_tokens)\n",
        "unique_tokens = len(set(all_tokens))\n",
        "ttr = total_tokens / unique_tokens if unique_tokens > 0 else 0\n",
        "\n",
        "# Lemma frequency\n",
        "lemma_freq = Counter([lemma for lemma in all_lemmas if lemma.strip()])\n",
        "\n",
        "# Report\n",
        "print(\"Tokenization and Lemmatization Report\")\n",
        "print(f\"Total tokens (excluding stopwords, punctuation, numbers): {total_tokens}\")\n",
        "print(f\"Unique tokens (types): {unique_tokens}\")\n",
        "print(f\"Token-to-Type Ratio (TTR): {ttr:.2f}\")\n",
        "\n",
        "print(\"\\nTop 20 Most Frequent Lemmas:\")\n",
        "for lemma, freq in lemma_freq.most_common(20):\n",
        "    print(f\"{lemma}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/_thesis_dogwhistles/Analyze-tweets/left-lemmas.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(all_lemmas))"
      ],
      "metadata": {
        "id": "A-8XiXEZJngE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30 most frequently used nouns and adjectives"
      ],
      "metadata": {
        "id": "8GmO0myulAEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load French spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Process content column\n",
        "texts = df['content'].dropna().astype(str).tolist()\n",
        "nouns = []\n",
        "adjectives = []\n",
        "\n",
        "for doc in nlp.pipe(texts, batch_size=500, disable=[\"ner\", \"parser\"]):\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            if token.pos_ == \"NOUN\":\n",
        "                nouns.append(token.lemma_.lower())\n",
        "            elif token.pos_ == \"ADJ\":\n",
        "                adjectives.append(token.lemma_.lower())\n",
        "\n",
        "# Count frequencies\n",
        "noun_freq = Counter(nouns).most_common(30)\n",
        "adj_freq = Counter(adjectives).most_common(30)\n",
        "\n",
        "# Display results\n",
        "print(\"üìö Top 30 Most Frequent Nouns:\")\n",
        "for word, freq in noun_freq:\n",
        "    print(f\" - {word}: {freq}\")\n",
        "\n",
        "print(\"\\nüé® Top 30 Most Frequent Adjectives:\")\n",
        "for word, freq in adj_freq:\n",
        "    print(f\" - {word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h62Rlu80k-Yh",
        "outputId": "7779bf53-befd-46c1-b1ad-259ecbbb7b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Top 30 Most Frequent Nouns:\n",
            " - an: 4261\n",
            " - ü§£: 3956\n",
            " - monde: 3541\n",
            " - oui: 3467\n",
            " - jour: 3378\n",
            " - droite: 2944\n",
            " - pays: 2819\n",
            " - temps: 2745\n",
            " - gauche: 2645\n",
            " - vie: 2547\n",
            " - fois: 2516\n",
            " - femme: 2488\n",
            " - ann√©e: 2438\n",
            " - droit: 2400\n",
            " - enfant: 2342\n",
            " - c: 2280\n",
            " - question: 2249\n",
            " - Ô∏è: 2241\n",
            " - chose: 2207\n",
            " - histoire: 2192\n",
            " - place: 2138\n",
            " - travail: 1957\n",
            " - probl√®me: 1813\n",
            " - personne: 1799\n",
            " - homme: 1789\n",
            " - √©tat: 1786\n",
            " - fran√ßais: 1762\n",
            " - moment: 1732\n",
            " - guerre: 1708\n",
            " - cas: 1672\n",
            "\n",
            "üé® Top 30 Most Frequent Adjectives:\n",
            " - bon: 5702\n",
            " - grand: 3481\n",
            " - petit: 2743\n",
            " - fran√ßais: 2463\n",
            " - nouveau: 2077\n",
            " - dernier: 2062\n",
            " - politique: 1992\n",
            " - public: 1919\n",
            " - extr√™me: 1905\n",
            " - gros: 1683\n",
            " - social: 1328\n",
            " - bel: 1274\n",
            " - vrai: 1268\n",
            " - europ√©en: 1024\n",
            " - üò≠: 1002\n",
            " - premier: 1001\n",
            " - meilleur: 998\n",
            " - fait: 975\n",
            " - national: 971\n",
            " - prochain: 960\n",
            " - s√ªr: 948\n",
            " - faux: 896\n",
            " - cher: 867\n",
            " - international: 829\n",
            " - üëâ: 813\n",
            " - jeune: 765\n",
            " - grave: 728\n",
            " - important: 722\n",
            " - ü§£: 710\n",
            " - simple: 701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re # Import the regular expressions library\n",
        "\n",
        "# Define the file paths\n",
        "file_path_c1 = '/content/drive/MyDrive/_thesis_dogwhistles/Compare-embeddings/corpora/c1-left.csv'\n",
        "file_path_c2 = '/content/drive/MyDrive/_thesis_dogwhistles/Compare-embeddings/corpora/c2-right.csv'\n",
        "\n",
        "try:\n",
        "    df_c1 = pd.read_csv(file_path_c1)\n",
        "    df_c2 = pd.read_csv(file_path_c2)\n",
        "\n",
        "    # --- Define a function for clean word counting ---\n",
        "    # This regex finds sequences of French/English letters.\n",
        "    # It will ignore numbers, emojis, and standalone punctuation.\n",
        "    def count_clean_words(text):\n",
        "        if not isinstance(text, str):\n",
        "            return 0\n",
        "        # For French, we include √Ä-√ø to catch accented characters\n",
        "        words = re.findall(r'\\b[a-zA-Z√Ä-√ø]+\\b', text.lower())\n",
        "        return len(words)\n",
        "\n",
        "    # --- Calculate clean word count for both corpora ---\n",
        "    total_words_c1 = df_c1['content'].apply(count_clean_words).sum()\n",
        "    total_words_c2 = df_c2['content'].apply(count_clean_words).sum()\n",
        "\n",
        "\n",
        "    # --- Print the results ---\n",
        "    print(\"Clean Corpus Word Count Results üí°\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"Total words in c1 (neutral corpus): {total_words_c1}\")\n",
        "    print(f\"Total words in c2 (ideological corpus): {total_words_c2}\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: A file was not found at {e.filename}\")\n",
        "except KeyError:\n",
        "    print(\"‚ùå Error: A column named 'content' was not found in one of the CSV files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrVlfNoo9UIR",
        "outputId": "a8cb199f-73f3-48d6-87d8-f344e57e94b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Corpus Word Count Results üí°\n",
            "-----------------------------------\n",
            "Total words in c1 (neutral corpus): 3343187\n",
            "Total words in c2 (ideological corpus): 3426165\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yA7way7J-Kfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}