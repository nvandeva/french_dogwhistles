# -*- coding: utf-8 -*-
"""tokenize-tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nnt8G3t-Z_lD1WkmjDbTJeTTBwoxslfj

- Remove punctuation and stop words
- Tokenize the cleaned tweets
- Compute token count, type count, and token-to-type ratio (TTR)
- Lemmatize each token
- Count lemma frequency
- Show the 20 most common lemmas
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q spacy
!python -m spacy download fr_core_news_sm
import pandas as pd

import spacy
from collections import Counter

from collections import Counter
import pandas as pd
import spacy

# Load French spaCy model
nlp = spacy.load("fr_core_news_sm")

# Read cleaned French tweets
df = pd.read_csv('/content/drive/My Drive/_thesis_dogwhistles/Analyze-tweets/left-cleaned-french-tweets.csv')

# Prepare text for spaCy processing
texts = df['content'].dropna().astype(str).tolist()

# Tokenize, remove stopwords, punctuation, numbers, and lemmatize
all_tokens = []
all_lemmas = []

for doc in nlp.pipe(texts, disable=["ner", "parser"]):
    for token in doc:
        if token.is_alpha and not token.is_stop:
            all_tokens.append(token.text.lower())
            all_lemmas.append(token.lemma_.lower())

# Count tokens, types, TTR
total_tokens = len(all_tokens)
unique_tokens = len(set(all_tokens))
ttr = total_tokens / unique_tokens if unique_tokens > 0 else 0

# Lemma frequency
lemma_freq = Counter([lemma for lemma in all_lemmas if lemma.strip()])

# Report
print("Tokenization and Lemmatization Report")
print(f"Total tokens (excluding stopwords, punctuation, numbers): {total_tokens}")
print(f"Unique tokens (types): {unique_tokens}")
print(f"Token-to-Type Ratio (TTR): {ttr:.2f}")

print("\nTop 20 Most Frequent Lemmas:")
for lemma, freq in lemma_freq.most_common(20):
    print(f"{lemma}: {freq}")

with open('/content/drive/My Drive/_thesis_dogwhistles/Analyze-tweets/left-lemmas.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(all_lemmas))

"""30 most frequently used nouns and adjectives"""

import spacy
from collections import Counter

# Load French spaCy model
nlp = spacy.load("fr_core_news_sm")

# Process content column
texts = df['content'].dropna().astype(str).tolist()
nouns = []
adjectives = []

for doc in nlp.pipe(texts, batch_size=500, disable=["ner", "parser"]):
    for token in doc:
        if not token.is_stop and not token.is_punct:
            if token.pos_ == "NOUN":
                nouns.append(token.lemma_.lower())
            elif token.pos_ == "ADJ":
                adjectives.append(token.lemma_.lower())

# Count frequencies
noun_freq = Counter(nouns).most_common(30)
adj_freq = Counter(adjectives).most_common(30)

# Display results
print("üìö Top 30 Most Frequent Nouns:")
for word, freq in noun_freq:
    print(f" - {word}: {freq}")

print("\nüé® Top 30 Most Frequent Adjectives:")
for word, freq in adj_freq:
    print(f" - {word}: {freq}")

import pandas as pd
import re # Import the regular expressions library

# Define the file paths
file_path_c1 = '/content/drive/MyDrive/_thesis_dogwhistles/Compare-embeddings/corpora/c1-left.csv'
file_path_c2 = '/content/drive/MyDrive/_thesis_dogwhistles/Compare-embeddings/corpora/c2-right.csv'

try:
    df_c1 = pd.read_csv(file_path_c1)
    df_c2 = pd.read_csv(file_path_c2)

    # --- Define a function for clean word counting ---
    # This regex finds sequences of French/English letters.
    # It will ignore numbers, emojis, and standalone punctuation.
    def count_clean_words(text):
        if not isinstance(text, str):
            return 0
        # For French, we include √Ä-√ø to catch accented characters
        words = re.findall(r'\b[a-zA-Z√Ä-√ø]+\b', text.lower())
        return len(words)

    # --- Calculate clean word count for both corpora ---
    total_words_c1 = df_c1['content'].apply(count_clean_words).sum()
    total_words_c2 = df_c2['content'].apply(count_clean_words).sum()


    # --- Print the results ---
    print("Clean Corpus Word Count Results üí°")
    print("-" * 35)
    print(f"Total words in c1 (neutral corpus): {total_words_c1}")
    print(f"Total words in c2 (ideological corpus): {total_words_c2}")
    print("-" * 35)

except FileNotFoundError as e:
    print(f"‚ùå Error: A file was not found at {e.filename}")
except KeyError:
    print("‚ùå Error: A column named 'content' was not found in one of the CSV files.")

